<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>OpenTouch ‚Äî Project Page</title>
  <meta name="description" content="OpenTouch dataset: synchronized egocentric video, full-hand tactile, and pose. Demos, protocol, analysis, and benchmarks." />

  <!-- Model Viewer (interactive 3D) -->
  <script type="module" src="https://unpkg.com/@google/model-viewer/dist/model-viewer.min.js"></script>
  <script nomodule src="https://unpkg.com/@google/model-viewer/dist/model-viewer-legacy.js"></script>

  <style>
    :root{
      --bg0:#070A12;
      --bg1:#0A1020;
      --line:rgba(255,255,255,.12);
      --line2:rgba(255,255,255,.08);
      --text:rgba(255,255,255,.92);
      --muted:rgba(255,255,255,.68);
      --faint:rgba(255,255,255,.50);
      --shadow: 0 18px 60px rgba(0,0,0,.45);
      --r2:24px;
      --max:1180px;
      --mono: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono","Courier New", monospace;
      --sans: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial;
      --a1:#7C3AED;
      --a2:#22D3EE;
    }

    *{box-sizing:border-box}
    html,body{height:100%}
    body{
      margin:0;
      font-family:var(--sans);
      color:var(--text);
      background:
        radial-gradient(1200px 650px at 20% -10%, rgba(124,58,237,.24), transparent 58%),
        radial-gradient(1100px 600px at 85% 5%, rgba(34,211,238,.20), transparent 60%),
        linear-gradient(180deg, var(--bg0), var(--bg1));
      overflow-x:hidden;
    }
    a{color:inherit}
    .wrap{max-width:var(--max);margin:0 auto;padding:0 18px}
    .glow{
      position:fixed; inset:-20vh -10vw auto -10vw;
      height:35vh; pointer-events:none; z-index:0;
      background: radial-gradient(closest-side, rgba(124,58,237,.18), transparent 70%);
      filter: blur(14px);
      opacity:.9;
    }

    /* Header */
    header{
      position:sticky; top:0; z-index:30;
      backdrop-filter: blur(14px);
      background: linear-gradient(180deg, rgba(7,10,18,.86), rgba(7,10,18,.55));
      border-bottom:1px solid var(--line2);
    }
    .nav{
      display:flex; align-items:center; justify-content:space-between;
      gap:14px; padding:12px 0;
    }
    .brand{display:flex; align-items:center; gap:10px; text-decoration:none; min-width:220px;}
    .logo{
      width:34px;height:34px;border-radius:12px;
      background: conic-gradient(from 210deg, var(--a1), var(--a2), var(--a1));
      border:1px solid rgba(255,255,255,.18);
      box-shadow: 0 10px 30px rgba(124,58,237,.20);
    }
    .brand b{display:block;font-size:18px;letter-spacing:.2px}
    .brand span{display:block;font-size:12px;color:var(--muted);margin-top:2px}
    .links{display:flex;gap:10px;flex-wrap:wrap;justify-content:flex-end}
    .pill{
      font-size:12px; color:var(--muted); text-decoration:none;
      padding:8px 10px; border-radius:999px;
      border:1px solid transparent;
      background: rgba(255,255,255,.03);
      transition: transform .12s ease, border-color .12s ease, color .12s ease;
      user-select:none;
      white-space:nowrap;
    }
    .pill:hover{transform:translateY(-1px); color:var(--text); border-color:var(--line)}
    .pill.primary{
      color:rgba(255,255,255,.88);
      border-color: rgba(34,211,238,.32);
      background: linear-gradient(135deg, rgba(124,58,237,.16), rgba(34,211,238,.12));
    }

    main{position:relative; z-index:1}
    section{padding:24px 0}
    .hero{padding:30px 0 16px}

    .title{
      font-size: clamp(24px, 3.2vw, 44px);
      line-height:1.08;
      letter-spacing:-.8px;
      margin: 6px 0 10px;
    }
    .authors{
      color:var(--muted);
      font-size: 16px;
      line-height:1.45;
      margin: 0 0 10px;
      max-width: 95ch;
    }
    .affil{
      color:var(--faint);
      font-size: 18px;
      margin: 0 0 16px;
      max-width: 95ch;
    }
    .cta{display:flex; gap:10px; flex-wrap:wrap; margin: 14px 0 6px;}
    .btn{
      display:inline-flex; align-items:center; gap:10px;
      padding:10px 12px; border-radius:14px;
      border:1px solid var(--line);
      background: rgba(255,255,255,.04);
      text-decoration:none;
      color:rgba(255,255,255,.86);
      transition: transform .12s ease, border-color .12s ease, background .12s ease;
      user-select:none;
    }
    .btn:hover{transform:translateY(-1px); border-color: rgba(34,211,238,.40); background: rgba(255,255,255,.06)}
    .btn.primary{
      border-color: transparent;
      background: linear-gradient(135deg, rgba(124,58,237,.95), rgba(34,211,238,.90));
      color:#06101a;
      font-weight: 750;
    }

    .secHead{
      display:flex; align-items:flex-end; justify-content:space-between;
      gap:12px; margin: 6px 0 12px;
    }
    .secHead h2{margin:0;font-size: 24px;letter-spacing:-.2px;}
    .secHead p{margin:0; color:var(--muted); font-size: 12.5px}

    .card{
      border:1px solid var(--line2);
      background: linear-gradient(180deg, rgba(255,255,255,.06), rgba(255,255,255,.035));
      border-radius: var(--r2);
      box-shadow: var(--shadow);
      overflow:hidden;
    }
    .pad{padding:16px}

    .frame{
      border-radius: 20px;
      border:1px solid rgba(255,255,255,.14);
      overflow:hidden;
      background: rgba(0,0,0,.25);
    }
    .cap{
      padding:10px 12px;
      border-top:1px solid var(--line2);
      color:var(--muted);
      font-size: 12.5px;
      display:flex; justify-content:space-between; gap:10px; flex-wrap:wrap;
      background: rgba(255,255,255,.02);
    }

    /* Single-media blocks: full width, keep original aspect */
    .frame.auto img,
    .frame.auto video{
      width:100%;
      height:auto;
      display:block;
      object-fit: contain;
      object-position:center;
      background: rgba(0,0,0,.25);
    }

    /* Examples: side-by-side => same height, keep aspect (letterbox ok) */
    .rows{display:flex;flex-direction:column;gap:12px}
    .row{
      display:grid;
      grid-template-columns: 1.35fr .85fr;
      gap:12px;
      align-items: stretch;
    }
    .row .frame{height: clamp(220px, 26vw, 340px)}
    #examples .row .frame img,
    #examples .row .frame video{
      width:100%;
      height:100%;
      object-fit: contain;
      display:block;
      background: rgba(0,0,0,.25);
    }
    @media (max-width: 980px){
      .row{grid-template-columns: 1fr}
      .row .frame{height:auto;}
      #examples .row .frame img,
      #examples .row .frame video{height:auto;}
    }

    /* ===== PROTOCOL: left square viewer, right rectangle matching PNG aspect, same height ===== */
    .protoRow{
      --gap: 12px;
      --protoH: 360px;       /* JS sets */
      --pipeAspect: 2.0;     /* JS sets = imgW/imgH */
      display:flex;
      gap: var(--gap);
      align-items: stretch;
      width:100%;
    }

    /* Each side is a "frame" but we split into media + cap so cap doesn't steal height */
    .protoViewer{
      flex: 0 0 var(--protoH);
      height: var(--protoH);
      display:flex;
      flex-direction:column;
      min-width:0;
    }
    .protoPipe{
      flex: 1 1 auto;           /* take remaining width */
      height: var(--protoH);
      display:flex;
      flex-direction:column;
      min-width:0;
    }

    /* Media areas: these are what must match height */
    .protoMedia{
      flex: 1 1 auto;
      min-height:0;
      width:100%;
      display:block;
    }

    /* Viewer fills its media area */
    .protoViewer .protoMedia model-viewer{
      width:100%;
      height:100%;
      display:block;
    }

    /* Pipeline media area keeps the PNG aspect */
    .protoPipe .protoMedia{
      aspect-ratio: var(--pipeAspect);
      height: 100%;
    }

    /* Image fits without distortion */
    .protoPipe .protoMedia img{
      width:100%;
      height:100%;
      object-fit: contain;
      display:block;
      background: rgba(0,0,0,.25);
    }

    /* model-viewer styling */
    model-viewer{
      background: radial-gradient(600px 400px at 50% 20%, rgba(34,211,238,.10), transparent 60%),
                  rgba(0,0,0,.18);
    }
    .mvWrap{position:relative}
    .mvCap{
      position:absolute; left:12px; top:12px;
      padding:6px 10px;
      border-radius: 999px;
      font-size: 12px;
      border:1px solid rgba(255,255,255,.18);
      background: rgba(0,0,0,.35);
      color: rgba(255,255,255,.88);
      backdrop-filter: blur(10px);
      z-index:2;
    }

    /* Mobile: stack; viewer stays square; pipeline keeps aspect */
    @media (max-width: 980px){
      .protoRow{flex-direction:column;}
      .protoViewer, .protoPipe{height:auto; flex: initial; width:100%;}
      .protoViewer .protoMedia{aspect-ratio: 1 / 1;}
      .protoPipe .protoMedia{height:auto; aspect-ratio: var(--pipeAspect);}
    }

    .mini{color:var(--muted); font-size: 12.5px; margin: 10px 0 0}
    .badge{
      display:inline-flex; align-items:center; gap:8px;
      padding:4px 10px;
      border-radius:999px;
      border:1px solid rgba(34,211,238,.26);
      background: rgba(34,211,238,.06);
      color: rgba(255,255,255,.86);
      font-size: 12px;
      margin-left: 8px;
    }
    .findings{
      margin: 10px 0 0;
      padding-left: 18px;
      color: var(--muted);
      font-size: 18px;
      line-height: 1.55;
    }

    /* 2x2 images for grasp taxonomy examples (if you later add them) */
    .grid2x2{display:grid;grid-template-columns: 1fr 1fr;gap:12px;}
    @media (max-width:980px){ .grid2x2{grid-template-columns:1fr;} }
    .grid2x2 .frame{aspect-ratio: 1/1;}
    .grid2x2 .frame img{
      width:100%;
      height:100%;
      object-fit: contain;
      display:block;
      background: rgba(0,0,0,.25);
    }

    /* Bench layout: left column (videos stacked), right column (tables) */
    .benchGrid{
      display:grid;
      grid-template-columns: 1fr 1fr;
      gap:12px;
      align-items:start;
    }
    @media (max-width:980px){ .benchGrid{grid-template-columns:1fr;} }
    .benchLeft{
      display:flex;
      flex-direction:column;
      gap:12px;
    }
    .benchLeft .frame{
      height: clamp(220px, 22vw, 320px);
    }
    .benchLeft video{
      width:100%;
      height:100%;
      object-fit: contain;
      display:block;
      background: rgba(0,0,0,.25);
    }

    table{
      width:100%;
      border-collapse: collapse;
      border:1px solid var(--line2);
      background: rgba(255,255,255,.02);
      border-radius: 18px;
      overflow:hidden;
    }
    th, td{
      padding:10px 10px;
      border-bottom:1px solid var(--line2);
      text-align:left;
      font-size: 18px;
      vertical-align: top;
      color: rgba(255,255,255,.86);
    }
    th{color:var(--muted); font-weight: 700; background: rgba(255,255,255,.03)}
    tr:last-child td{border-bottom:none}

    pre{
      margin:0;
      padding:14px;
      border-radius: 18px;
      border:1px solid var(--line2);
      background: rgba(0,0,0,.22);
      overflow:auto;
      font-family: var(--mono);
      font-size: 12.5px;
      color: rgba(255,255,255,.88);
      line-height:1.4;
    }
    .rowEnd{
      display:flex; gap:10px; flex-wrap:wrap; align-items:center; justify-content:space-between;
      margin-top: 12px;
    }
    .copy{
      cursor:pointer;
      padding:9px 12px;
      border-radius: 14px;
      border:1px solid var(--line);
      background: rgba(255,255,255,.04);
      color: rgba(255,255,255,.86);
      user-select:none;
    }
    .copy:hover{border-color: rgba(34,211,238,.40)}
    footer{
      padding: 26px 0 40px;
      border-top:1px solid var(--line2);
      color: var(--muted);
      font-size: 12.5px;
      margin-top: 16px;
    }

    .likeBox{display:flex; gap:10px; flex-wrap:wrap; align-items:center;justify-content:space-between;}
    .likeBtn{
      cursor:pointer;
      display:inline-flex; align-items:center; gap:10px;
      padding:10px 12px; border-radius: 14px;
      border:1px solid rgba(255,255,255,.16);
      background: linear-gradient(135deg, rgba(124,58,237,.16), rgba(34,211,238,.10));
      color: rgba(255,255,255,.90);
      user-select:none;
    }
    .likeBtn:hover{border-color: rgba(34,211,238,.45)}
    .likeCount{
      font-family: var(--mono);
      color: rgba(255,255,255,.85);
      padding: 2px 8px;
      border-radius: 999px;
      border:1px solid rgba(255,255,255,.14);
      background: rgba(0,0,0,.20);
    }
  
    /* In-the-wild: 3 videos stacked vertically, full width */
    .itwCol{
      display:flex;
      flex-direction:column;
      gap:12px;
    }
    .itwCol .frame video{
      width:100%;
      height:auto;            /* keep original aspect */
      display:block;
      object-fit: contain;
      background: rgba(0,0,0,.25);
    }


  </style>
</head>

<body>
<div class="glow"></div>

<header>
  <div class="wrap">
    <div class="nav">
      <a class="brand" href="#top">
        <div class="logo" aria-hidden="true"></div>
        <div>
          <b>OpenTouch</b>
          <span>Dataset ‚Ä¢ Protocol ‚Ä¢ Hardware ‚Ä¢ Analysis ‚Ä¢ Benchmarks</span>
        </div>
      </a>
      <nav class="links" aria-label="Section navigation">
        <a class="pill" href="#teaser">Teaser</a>
        <a class="pill" href="#examples">Examples</a>
        <a class="pill" href="#protocol">Protocol</a>
        <a class="pill" href="#analysis_tax">Analysis</a>
        <a class="pill" href="#ego4d">Ego4D</a>
        <a class="pill primary" href="#bench">Benchmarks</a>
        <a class="pill" href="#cite">Citation</a>
        <a class="pill" href="#contact">Contact</a>
      </nav>
    </div>
  </div>
</header>

<main id="top">
  <!-- HERO -->
  <section class="hero">
    <div class="wrap">
      <div class="card pad">
        <div class="title">OpenTouch: Bringing Full-Hand Touch to Real-World Interaction</div>

        <p class="authors">
          <b>Yuxin Ray Song</b><sup>1,*</sup>&nbsp;&nbsp;
          <b>Jinzhou Li</b><sup>2,*</sup>&nbsp;&nbsp;
          <b>Rao Fu</b><sup>3,*</sup>&nbsp;&nbsp;
          <b>Devin Murphy</b><sup>4</sup>&nbsp;&nbsp;
          <b>Kaichen Zhou</b><sup>1,5</sup><br/>
          <b>Rishi Shiv</b><sup>1</sup>&nbsp;&nbsp;
          <b>Yaqi Li</b><sup>1</sup>&nbsp;&nbsp;
          <b>Haoyu Xiong</b><sup>1</sup>&nbsp;&nbsp;
          <b>Crystal E. Owens</b><sup>1</sup>&nbsp;&nbsp;
          <b>Yilun Du</b><sup>5</sup><br/>
          <b>Yiyue Luo</b><sup>4</sup>&nbsp;&nbsp;
          <b>Xianyi Cheng</b><sup>2</sup>&nbsp;&nbsp;
          <b>Antonio Torralba</b><sup>1</sup>&nbsp;&nbsp;
          <b>Wojciech Matusik</b><sup>1</sup>&nbsp;&nbsp;
          <b>Paul Pu Liang</b><sup>1</sup>
        </p>

        <p class="affil">
          <sup>1</sup>MIT&nbsp;&nbsp;&nbsp;
          <sup>2</sup>Duke University&nbsp;&nbsp;&nbsp;
          <sup>3</sup>Brown University&nbsp;&nbsp;&nbsp;
          <sup>4</sup>University of Washington&nbsp;&nbsp;&nbsp;
          <sup>5</sup>Harvard University
          <br/>
          <span style="color:var(--muted); font-size:12.5px;"><sup>*</sup>Equal contribution</span>
        </p>

        <div class="cta">
            <a class="btn primary" href="#teaser">‚ñ∂ Watch Teaser</a>
            <a class="btn" href="https://arxiv.org/abs/2512.16842" target="_blank" rel="noopener">üìÑ Paper</a>
            <a class="btn" href="https://github.com/opentouch-tactile/opentouch" target="_blank" rel="noopener">üíª Code (Coming)</a>
            <a class="btn" href="https://opentouch.world/" target="_blank" rel="noopener">üß© Dataset (Coming)</a>
        </div>
      </div>
    </div>
  </section>

  <!-- TEASER -->
  <section id="teaser">
    <div class="wrap">
      <div class="secHead">
        <h2>OpenTouch: Egocentric Video ‚Ä¢ Full-hand Tactile ‚Ä¢ Hand Poses</h2>
        <p></p>
      </div>

      <div class="card pad">
        <div class="frame auto">
          <video autoplay muted loop playsinline preload="auto" controls poster="assets/posters/teaser.jpg">
            <source src="assets/videos/teaser.mp4" type="video/mp4" />
          </video>
        </div>

        <div class="pad" style="padding-top:12px;">
          <p style="margin:0; color:var(--muted); font-size:18px; line-height:1.65;">
            OPENTOUCH is the first in-the-wild, full-hand tactile dataset with synchronized egocentric video,
            force-aware full-hand touch, and hand-pose trajectories. It contains 5 hours of recordings, including
            3 hours of densely annotated, contact-rich interactions.
          </p>
        </div>
      </div>
    </div>
  </section>

  <!-- EXAMPLES -->
  <section id="examples">
    <div class="wrap">
      <div class="secHead">
        <h2>Example Vision-Touch-Pose Data</h2>
        <p></p>
      </div>

      <p class="mini" style="font-size:18px; line-height:1.7;">
        OPENTOUCH demonstrates that hardware-based tactile sensing and pose tracking reveal critical force,
        contact, and motion cues that vision alone cannot capture.
      </p>

      <div class="card pad">
        <div class="rows">
          <div class="row">
            <div class="frame">
              <img src="assets/examples/ex1.png" alt="Example 1 figure" />
              <div class="cap"><span>Example 1 ‚Äî figure</span><span>assets/examples/ex1.png</span></div>
            </div>
            <div class="frame">
              <video autoplay muted loop playsinline preload="metadata" controls poster="assets/examples/ex1_poster.jpg">
                <source src="assets/examples/ex1.mp4" type="video/mp4" />
              </video>
              <div class="cap"><span>Example 1 ‚Äî video</span><span>assets/examples/ex1.mp4</span></div>
            </div>
          </div>
          <p class="mini" style="font-size:18px; line-height:1.7;">
            (a) Although the first three frames show nearly identical hand poses, the tactile signals reveal that in the third frame the hand applies sufficient force to move the chair.
          </p>

          <div class="row">
            <div class="frame">
              <img src="assets/examples/ex2.png" alt="Example 2 figure" />
              <div class="cap"><span>Example 2 ‚Äî figure</span><span>assets/examples/ex2.png</span></div>
            </div>
            <div class="frame">
              <video autoplay muted loop playsinline preload="metadata" controls poster="assets/examples/ex2_poster.jpg">
                <source src="assets/examples/ex2.mp4" type="video/mp4" />
              </video>
              <div class="cap"><span>Example 2 ‚Äî video</span><span>assets/examples/ex2.mp4</span></div>
            </div>
          </div>
          <p class="mini" style="font-size:18px; line-height:1.7;">
            (b) In the first frame, tactile readings clearly indicate contact with the table, ambiguous from RGB alone. In the next
            two frames, the hand moves out of view, making vision-based pose estimation unreliable; OPENTOUCH provides accurate
            hardware-tracked poses throughout.
          </p>

          <div class="row">
            <div class="frame">
              <img src="assets/examples/ex3.png" alt="Example 3 figure" />
              <div class="cap"><span>Example 3 ‚Äî figure</span><span>assets/examples/ex3.png</span></div>
            </div>
            <div class="frame">
              <video autoplay muted loop playsinline preload="metadata" controls poster="assets/examples/ex3_poster.jpg">
                <source src="assets/examples/ex3.mp4" type="video/mp4" />
              </video>
              <div class="cap"><span>Example 3 ‚Äî video</span><span>assets/examples/ex3.mp4</span></div>
            </div>
          </div>
          <p class="mini" style="font-size:18px; line-height:1.7;">
            (c) Tactile sensing exposes clear interaction patterns with transparent object that remain difficult to infer from visual tracking alone.
          </p>

          <div class="row">
            <div class="frame">
              <img src="assets/examples/ex4.png" alt="Example 4 figure" />
              <div class="cap"><span>Example 4 ‚Äî figure</span><span>assets/examples/ex4.png</span></div>
            </div>
            <div class="frame">
              <video autoplay muted loop playsinline preload="metadata" controls poster="assets/examples/ex4_poster.jpg">
                <source src="assets/examples/ex4.mp4" type="video/mp4" />
              </video>
              <div class="cap"><span>Example 4 ‚Äî video</span><span>assets/examples/ex4.mp4</span></div>
            </div>
          </div>
        </div>

        <p class="mini" style="font-size:18px; line-height:1.7;">
          (d) The tactile map captures a subtle middle-finger double-click on a button, a fine-grained motion that even pose tracking may miss.
          See the supplementary video for the high-fidelity tactile signals and subtle dynamic patterns.
        </p>
      </div>
    </div>
  </section>

  <!-- PROTOCOL -->
  <section id="protocol">
    <div class="wrap">
      <div class="secHead">
        <h2>Hardware + Annotation Setup</h2>
      </div>
      <p class="mini" style="font-size:18px; line-height:1.7;">
        Meta Aria glasses, Rokoko Smartgloves, and the FPC-based tactile sensor are synchronized at 30 Hz with an average 2 ms latency.
        High-level descriptions and detailed annotations are automatically generated from the egocentric video and the rendered tactile maps using a large language model.
      </p>

      <div class="card pad">
        <div class="protoRow" id="protocolRow">
          <!-- LEFT: viewer frame -->
          <div class="frame mvWrap protoViewer">
            <div class="mvCap">Interactive</div>

            <div class="protoMedia">
              <model-viewer
                src="assets/models/hand_origin.glb"
                alt="Hand mesh visualizer"
                camera-controls
                touch-action="pan-y"
                exposure="1.0"
                shadow-intensity="1"
                environment-image="neutral"
                camera-target="0m 0m 0m"
                auto-rotate
                auto-rotate-delay="3000">
              </model-viewer>
            </div>

            <div class="cap"><span>Full-hand Touch Spatial Layout</span></div>
          </div>

          <!-- RIGHT: pipeline frame -->
          <div class="frame protoPipe" id="pipelineBox">
            <div class="protoMedia">
              <img id="pipelineImg" src="assets/figs/annotation_pipeline.png" alt="Annotation pipeline figure" />
            </div>
            <div class="cap"><span>Annotation pipeline</span></div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- ANALYSIS: tactile map & taxonomy -->
  <section id="analysis_tax">
    <div class="wrap">
      <div class="secHead">
        <h2>Tactile Map & Grasp Taxonomy</h2>
      </div>
      <p class="mini" style="font-size:18px; line-height:1.7;">
        We visualize accumulated tactile maps across dataset for different grasp types (defined by grasp taxonomy).
        The spatial pressure patterns strongly correlate with the underlying grasp configuration,
        demonstrating the accuracy and quality of our tactile data and grasp type annotation.
      </p>
      <div class="card pad">
        <div class="frame auto">
          <img src="assets/figs/tactile_taxonomy.png" alt="Tactile map and taxonomy figure" />
        </div>
      </div>
      <div style="height:12px"></div>
    </div>
  </section>

  <!-- ANALYSIS: statistics -->
  <section id="analysis_stats">
    <div class="wrap">
      <div class="secHead">
        <h2>Annotation Statistics</h2>
      </div>
      <p class="mini" style="font-size:18px; line-height:1.7;">
        Sankey diagram visualizing the distribution of dataset labels, including environment, action, grasp type, and object category.
        In total, OpenTouch contains objects across 14 everyday environments, covering over 8,000 objects from 14 categories.
      </p>
      <div class="card pad">
        <div class="frame auto">
          <img src="assets/figs/stats_sankey.png" alt="Sankey diagram + stats figure" />
        </div>
      </div>
      <div style="height:12px"></div>
    </div>
  </section>

  <section id="ego4d">
    <div class="wrap">
      <div class="secHead">
        <h2>Tactile retrieval in-the-wild (Ego4D)</h2>
      </div>
      <p class="mini" style="font-size:18px; line-height:1.7;">
       OPENTOUCH can act as a tactile database for in-the-wild egocentric video datasets: we demonstrate that in-the-wild video (e.g., Ego4D) can retrieve plausible tactile sequences, enabling large-scale egocentric video to be augmented with contact and force cues.
       The source videos paired with the retrieved tactile exhibit human behaviors and manipulation primitives strikingly similar to the query.
      </p>
    <div class="card pad">
      <div class="itwCol">
        <div class="frame auto">
          <video autoplay muted loop playsinline preload="metadata" controls>
            <source src="assets/videos/opentouch-itw-01.mp4" type="video/mp4" />
          </video>
          <div class="cap"><span>In-the-wild #1</span><span>opentouch-itw-01.mp4</span></div>
        </div>

        <div class="frame auto">
          <video autoplay muted loop playsinline preload="metadata" controls>
            <source src="assets/videos/opentouch-itw-02.mp4" type="video/mp4" />
          </video>
          <div class="cap"><span>In-the-wild #2</span><span>opentouch-itw-02.mp4</span></div>
        </div>

        <div class="frame auto">
          <video autoplay muted loop playsinline preload="metadata" controls>
            <source src="assets/videos/opentouch-itw-03.mp4" type="video/mp4" />
          </video>
          <div class="cap"><span>In-the-wild #3</span><span>opentouch-itw-03.mp4</span></div>
        </div>
      </div>
    </div>

    </div>
  </section>


  <!-- BENCHMARKS -->
  <section id="bench">
    <div class="wrap">
      <div class="secHead">
        <h2>OpenTouch benchmark</h2>
        <p>Retrieval + classification</p>
      </div>

      <div class="card pad">
        <div class="secHead" style="margin-top:0;">
          <h2>Retrieval benchmark</h2>
        </div>

        <div class="benchGrid">
          <!-- LEFT: two videos stacked -->
          <div class="benchLeft">
            <div class="frame">
              <video autoplay muted loop playsinline preload="metadata" controls poster="assets/posters/retrieval_t2v.jpg">
                <source src="assets/videos/retrieval_tactile_to_video.mp4" type="video/mp4" />
              </video>
              <div class="cap"><span>tactile ‚Üí video</span><span>assets/videos/retrieval_tactile_to_video.mp4</span></div>
            </div>

            <div class="frame">
              <video autoplay muted loop playsinline preload="metadata" controls poster="assets/posters/retrieval_v2t.jpg">
                <source src="assets/videos/retrieval_video_to_tactile.mp4" type="video/mp4" />
              </video>
              <div class="cap"><span>video ‚Üí tactile</span><span>assets/videos/retrieval_video_to_tactile.mp4</span></div>
            </div>
          </div>

          <!-- RIGHT: two tables -->
          <div>
            <div class="secHead" style="margin:0 0 10px;">
              <h2>Bi-modal (Ours only)</h2><p>R@1 / R@5 / R@10 / mAP</p>
            </div>

                <table aria-label="Bi-modal ours-only">
                <thead>
                    <tr><th>Direction</th><th>R@1</th><th>R@5</th><th>R@10</th><th>mAP</th></tr>
                </thead>
                <tbody>
                    <tr><td>video ‚Üí tactile</td><td>7.15</td><td>26.73</td><td>39.74</td><td>15.47</td></tr>
                    <tr><td>tactile ‚Üí video</td><td>7.15</td><td>26.30</td><td>39.03</td><td>15.28</td></tr>

                    <tr><td>pose ‚Üí tactile</td><td>6.93</td><td>21.02</td><td>30.45</td><td>13.13</td></tr>
                    <tr><td>tactile ‚Üí pose</td><td>7.15</td><td>21.87</td><td>30.88</td><td>13.43</td></tr>
                </tbody>
                </table>

            <div style="height:12px"></div>

            <div class="secHead" style="margin:0 0 10px;">
              <h2>Tri-modal (Ours only)</h2><p>R@1 / R@5 / R@10 / mAP</p>
            </div>

            <table aria-label="Tri-modal ours-only">
              <thead>
                <tr><th>Direction</th><th>R@1</th><th>R@5</th><th>R@10</th><th>mAP</th></tr>
              </thead>
              <tbody>
                <tr><td>video + pose ‚Üí tactile</td><td>14.08</td><td>42.96</td><td>62.26</td><td>26.86</td></tr>
                <tr><td>tactile + pose ‚Üí video</td><td>12.72</td><td>38.53</td><td>53.18</td><td>23.46</td></tr>
                <tr><td>video + tactile ‚Üí pose</td><td>15.44</td><td>43.39</td><td>57.61</td><td>26.86</td></tr>
              </tbody>
            </table>

            <ul class="findings">
              <li><b>Finding 1:</b> Bi-Modal retrieval. The symmetry across both directions suggests that learned representation is genuinely multimodal rather than biased toward a single modality.</li>
              <li><b>Finding 2:</b> Multi-modal outperform unimodal. Multimodal inputs offer complementary information that reduces retrieval ambiguity, as video provides global scene context, pose encodes kinematics, and tactile captures local contact and force.</li>
            </ul>
          </div>
        </div>
      </div>

      <div style="height:12px"></div>

      <div class="card pad">
        <div class="secHead" style="margin-top:0;">
          <h2>Classification benchmark</h2>
        </div>

        <table aria-label="Classification table">
          <thead>
            <tr>
              <th>Modality</th>
              <th>Action Acc. (RN18)</th>
              <th>Action Acc. (Lite-CNN)</th>
              <th>Grasp Acc. (RN18)</th>
              <th>Grasp Acc. (Lite-CNN)</th>
            </tr>
          </thead>
          <tbody>
            <tr><td>V</td><td>40.26</td><td>‚Äî</td><td>57.45</td><td>‚Äî</td></tr>
            <tr><td>P</td><td>33.22</td><td>‚Äî</td><td>46.32</td><td>‚Äî</td></tr>
            <tr><td>T</td><td>29.95</td><td>31.59</td><td>60.23</td><td>57.12</td></tr>
            <tr><td>T + P</td><td>28.31</td><td>27.00</td><td>60.72</td><td>62.19</td></tr>
            <tr><td>T + V</td><td>30.11</td><td>32.73</td><td>51.72</td><td>65.47</td></tr>
            <tr><td>T + P + V</td><td>35.02</td><td>37.32</td><td>55.65</td><td>68.09</td></tr>
          </tbody>
        </table>

        <ul class="findings">
          <li><b>Finding 1:</b> Tactile is highly informative for grasp type, reflecting that grasp relies on local contact geometry.</li>
          <li><b>Finding 2:</b> Actions recognition depend on higher-level global context provided by video modality.</li>
        </ul>
      </div>
    </div>
  </section>

  <!-- CITATION -->
  <section id="cite">
    <div class="wrap">
      <div class="secHead">
        <h2>Citation</h2>
        <p>BibTeX</p>
      </div>

      <div class="card pad">
        <pre id="bibtex">@misc{song2025opentouchbringingfullhandtouch,
      title={OPENTOUCH: Bringing Full-Hand Touch to Real-World Interaction},
      author={Yuxin Ray Song and Jinzhou Li and Rao Fu and Devin Murphy and Kaichen Zhou and Rishi Shiv and Yaqi Li and Haoyu Xiong and Crystal Elaine Owens and Yilun Du and Yiyue Luo and Xianyi Cheng and Antonio Torralba and Wojciech Matusik and Paul Pu Liang},
      year={2025},
      eprint={2512.16842},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2512.16842},
}</pre>

        <div class="rowEnd">
          <div class="mini" id="copyHint">Click to copy BibTeX.</div>
          <button class="copy" id="copyBtn">Copy BibTeX</button>
        </div>
      </div>
    </div>
  </section>

  <!-- ACKNOWLEDGMENT -->
  <section id="ack">
    <div class="wrap">
      <div class="secHead">
        <h2>Acknowledgment</h2>
        <p></p>
      </div>

      <div class="card pad">
        <p style="margin:0; color:var(--muted); font-size:12px; line-height:1.75;">
          We thank the MIT Office of Research Computing and Data (ORCD) for support through ORCD Seed Fund Grants,
          which provided access to 8&times;H200 GPUs and additional funding support. We also thank the NVIDIA
          Academic Grant Program for GPU support, Murata, and Analog Devices for supporting this work through the
          MIT Gen AI Impact Consortium. Any opinion, findings, and conclusions or recommendations expressed in this
          material are those of the authors and do not necessarily reflect the views of NVIDIA, Murata, and Analog Devices.
        </p>
      </div>
    </div>
  </section>

  <!-- CONTACT -->
  <section id="contact">
    <div class="wrap">
      <div class="secHead">
        <h2>Like & Contact</h2>
        <p>Say hi / request access / collaborations</p>
      </div>

      <div class="card pad">
        <div class="rowEnd">
          <div style="color:var(--muted);line-height:1.6;">
            <b style="color:rgba(255,255,255,.90)">Corresponding author:</b> Ray Song<br/>
            Email: <a href="mailto:rayxsong@mit.edu">rayxsong@mit.edu</a><br/>
            GitHub: <a href="https://github.com/opentouch-tactile">opentouch-tactile</a>
          </div>
          <a class="btn" href="mailto:rayxsong@mit.edu?subject=OpenTouch%20Project%20Page">‚úâÔ∏è Email</a>
        </div>
      </div>
    </div>
  </section>

  <footer>
    <div class="wrap">
      <div> Webpage developed by Rao Fu <a href="mailto:rao_fu@brown.edu">rao_fu@brown.edu</a>.</div>
    </div>
  </footer>
</main>

<script>
  // Smooth scroll for internal links
  document.querySelectorAll('a[href^="#"]').forEach(a => {
    a.addEventListener('click', (e) => {
      const id = a.getAttribute('href');
      const el = document.querySelector(id);
      if (!el) return;
      e.preventDefault();
      el.scrollIntoView({ behavior: "smooth", block: "start" });
      history.pushState(null, "", id);
    });
  });

  // Pause autoplay videos when offscreen (saves CPU)
  const vids = [...document.querySelectorAll('video')];
  const io = new IntersectionObserver((entries) => {
    entries.forEach(e => {
      const v = e.target;
      const isAuto = v.hasAttribute('autoplay');
      if (!isAuto) return;
      if (e.isIntersecting) v.play().catch(()=>{});
      else v.pause();
    });
  }, { threshold: 0.15 });
  vids.forEach(v => io.observe(v));

  // Copy BibTeX
  const copyBtn = document.getElementById('copyBtn');
  const bibtex = document.getElementById('bibtex');
  const copyHint = document.getElementById('copyHint');
  copyBtn?.addEventListener('click', async () => {
    try{
      await navigator.clipboard.writeText(bibtex.innerText);
      copyHint.textContent = "Copied!";
      setTimeout(()=> copyHint.textContent = "Click to copy BibTeX.", 1200);
    }catch{
      copyHint.textContent = "Copy failed. Select the text manually.";
    }
  });

  // Protocol sizing:
  // viewer is HxH, pipeline media is H x (H*aspectPNG), and (H + gap + H*aspectPNG) = containerWidth
  (function(){
    const row = document.getElementById('protocolRow');
    const img = document.getElementById('pipelineImg');
    if (!row || !img) return;

    const GAP_PX = 12; // must match CSS gap

    function computeAndSet(){
      if (window.matchMedia('(max-width: 980px)').matches) return;

      const w = row.clientWidth;
      const aspect = (img.naturalWidth && img.naturalHeight) ? (img.naturalWidth / img.naturalHeight) : 2.0;

      const H = (w - GAP_PX) / (1 + aspect);

      row.style.setProperty('--pipeAspect', String(aspect));
      row.style.setProperty('--protoH', `${H}px`);
    }

    if (img.complete) computeAndSet();
    else img.addEventListener('load', computeAndSet, { once:true });

    let raf = 0;
    window.addEventListener('resize', () => {
      cancelAnimationFrame(raf);
      raf = requestAnimationFrame(computeAndSet);
    });
  })();
</script>
</body>
</html>
